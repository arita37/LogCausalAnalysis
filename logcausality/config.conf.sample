
[general]

# Source log data path
# Less priority than command option of log_db
src_path = test.temp

# Search source path recursively
# If false, only search right under the given directory
src_recur = false

# Processing log output path (not dataset but log of this system)
# If blank, output log on stderr
info_log = auto.log


[database]

# Database management system to use
# [sqlite3, #mysql(debugging)] is available
# mysql : Require MySQL for PYTHON (MySQLdb) package
database = sqlite3

# Classified log database for sqlite3
sqlite3_filename = log.db

# Database hostname for mysql
mysql_host = localhost

# Database name for mysql
mysql_dbname = logcausality

# Mysql username
mysql_user = test

# Mysql user password
mysql_passwd = test

# Store log data with following splitter symbol string
# If log_template.sym_ignore is False,
# use symbol that will not appear in raw log messages
split_symbol = @@

# Network area groups of host names
# For exapmle:
# area_filename = area_def.txt
# If empty, no group defined
area_filename = 

# If no year string in log message, add following year
# (Use this year of localtime with empty value)
# For example:
# default_year = 2112
default_year = 

# Remove log messages with given message headers in following files
# For example to remove messages with free-write options
# remove_header_filename = freewrite_header.txt, hoge.txt
remove_header_filename = 


[log_template]

# 1st step algorithms / methods to generate log templates
# [shiso, va] are available
lt_alg = shiso

# 2nd step algorithms / methods
# especially for classifying log templates with different length
# [shiso, none] are available
# (none : with no grouping)
ltgroup_alg = shiso

# Output filename of internal data for log template generation
# Do NOT share among multiple log template generation algorithms
indata_filename = lt.dump

# Definition file of symbol strings to split log messages
# If empty, use default rule (symdef.txt.sample)
sym_filename = 

# Ignore splitting symbol strings in log template generation
# In many cases this option enables speeding up in exchange for precision
sym_ignore = true

# Symbol string to abstract variable strings in log templates
# that must not appear in raw log messages
variable_symbol = **


[log_template_import]

# Log template definition file path
def_path = 

# Log template definition file style
# [plain] is available
# plain : without headers(datetime and host)
mode = plain


[log_template_va]

# Source data of frequency dictionary
# Aiming to use past log data (before source of DB) as training data
# If empty, use source data of DB (defined in config, not command line option!)
src_path = 

# Incremental update of frequency dictionary
# If source data of frequency dictionary is equal to that of DB construction,
# Avoid double count of frequency with this option being false
incre_update = false

# Algorithm to decide frequency threshold
# [median, ] is available
threshold_mode = median

# Threshold value
# If mode is median, threshold means ratio of description words to all words
threshold = 0.6


[log_template_shiso]

# Threshold for SeqRatio in Search Phase
ltgen_threshold = 0.9

# Max child size of 1 node of tree in Search Phase
ltgen_max_child = 4

# Size of Ngram in Adjustment Phase
# If not ignoring splitter symbols, recommended to set more than 5
ltgroup_ngram_length = 3

# Lookup threshold for Ngram in Adjustment Phase
ltgroup_th_lookup = 0.3

# Threshold for edit distance in Adjustment Phase
ltgroup_th_distance = 0.85

# Keep found ngram database on memory
ltgroup_mem_ngram = true


[dag]

# Target term of log messages to construct DAG
# Divided into unit terms by "default_term" before processing
# If empty, set whole days including all log data in DB
# For example:
# whole_term = 2112-09-01 00:00:00, 2112-10-01 00:00:00
whole_term = 

# Target areas of DAG construction
# If "all" given, use whole hosts as 1 area named "all"
# If "each" given, define each host as 1 area named with hostname
# For example:
# area = core, area1, area2, area3
area = all

# Length of unit terms to construct DAG
unit_term = 30h

# Length of time difference of unit terms
unit_diff = 24h

# Bin size of discrete data for G square test
stat_bin = 10s

# Threshold of G square test to find conditional dependency
threshold = 0.01

# Found DAG object data
output_dir = pc_output

# Filtering rules to remove unnecessary events before generating causality graph
use_filter = 


[filter]
# Filters for log events
# [file, periodic, self-corr] is available
# file : Filtering IDs given in filter file
# periodic : Filtering events that appear with stable interval

# Filter filenames to use in filter option 'file'
# For example:
# use_filter = filter.txt, filter2.txt
filter_name = 

# Required times of periodic event appearance to repeat
# Used for 'periodic' and 'self-corr'
# set more than 3
count = 5

# Threshold value for filter 'periodic'
periodic_th = 0.5

# Required term length of periodic event to repeat
periodic_term = 6h

# Threshold value for self-correlation
self_corr_th = 0.5

# Difference duration candidates to calculate self-correlation
self_corr_diff = 1h, 1d

# Bin size to discretize event appearance data in calculating self-correlation
self_corr_bin = 10s


[visual]

# Log template label definition
# If empty, use default configuration (lt_label.conf.sample)
ltlabel = 


[cluster_graph]
# TODO to be moved to debugging repo,
# before graph matching is officially implemented

# Definition of distance calculation method
# [ed, mcs] is available
# ed : graph edit distance
# mcs : size ratio of maximum common subgraph
dist_method = ed

# Ignore direction of edge in graph or not
# when calculating distance of graphs
ig_direction = true

# Weight distance with TF-IDF
weight = true

# Criterion of threshold to generate flat cluster from hierarchy cluster
# [maxclust, distance, etc...]
th_criterion = maxclust

# Threshold (float value) to generate flat cluster from hierarchy cluster
th = 10.0

