
# Configuration file for logcausality
# If some options does not given in this file,
# logcausality will use defaults defined in config.conf.default


[general]

# Source log data path
# Less priority than command option of log_db
src_path = test.temp

# Search source path recursively
# If false, only search right under the given directory
src_recur = false

# Processing log output path (not dataset but log of this system)
# If empty, output log on stderr
info_log = auto.log

# Another configparser filename, to add any options without warnings
# Use this to define new options for private use
# In imported configparser file, existing options in defaults will be ignored
import = 


[database]

# Database management system to use
# [sqlite3, mysql] is available
# mysql : Require MySQL for PYTHON (MySQLdb) package
database = sqlite3

# Classified log database for sqlite3
sqlite3_filename = log.db

# Database hostname for mysql
mysql_host = localhost

# Database name for mysql
mysql_dbname = logcausality

# Mysql username
mysql_user = test

# Mysql user password
mysql_passwd = test

# Store log data with following splitter symbol string
# If log_template.sym_ignore is False,
# use symbol that will not appear in raw log messages
split_symbol = @@

# Network area groups of host names
# For exapmle:
# area_filename = area_def.txt
# If empty, no group defined
area_filename = 

# Hostname alias definition file
# Description example:
#   host 192.168.0.1 host.domain    <- host will be recorded in DB
# If empty, no host will be replaced
host_alias_filename = 

# Discard logs from undefined hosts in host_alias definition file
undefined_host = false

## A sequence of areas that cover all needed hosts.
## If a host does not belong to any hosts here,
## messages from the host is ignored.
#areas_to_register = 

# If no year string in log message, add following year
# (Use this year of localtime with empty value)
# For example:
# default_year = 2112
default_year = 

# Remove log messages with given message headers in following files
# For example to remove messages with free-write options
# remove_header_filename = freewrite_header.txt, hoge.txt
remove_header_filename = 


[log_template]

# 1st step algorithms / methods to generate log templates
# [shiso, va, import] are available
lt_alg = shiso

# 2nd step algorithms / methods
# especially for classifying log templates with different length
# [shiso, none] are available
# (none : with no grouping)
ltgroup_alg = shiso

# Post process algorithms / methods
# [host, dummy] are available
# Note : If you use postprocess to split variables,
#   it is not recommended to use ltgroup (ltgroup_alg = none)
post_alg = host, dummy

# Output filename of internal data for log template generation
# Do NOT share among multiple log template generation algorithms
indata_filename = lt.dump

# Output lines that fails to classify with existing log templates
# This can appear only if method "import" used
fail_output = lt_fail.log

# Definition file of symbol strings to split log messages
# If empty, use default rule (symdef.txt.sample)
sym_filename = 

# Ignore splitting symbol strings in log template generation
# In many cases this option enables speeding up in exchange for precision
sym_ignore = true

# Symbol string to abstract variable strings in log templates
# that must not appear in raw log messages
variable_symbol = **
labeled_variable_symbol_header = *
labeled_variable_symbol_footer = *


[log_template_import]

# Log template definition file path
def_path = 

# Log template definition file style
# [plain] is available
# plain : without headers(datetime and host)
mode = plain


[log_template_va]

# Source data of frequency dictionary
# Aiming to use past log data (before source of DB) as training data
# If empty, use source data of DB (defined in config, not command line option!)
src_path = 

# Incremental update of frequency dictionary
# If source data of frequency dictionary is equal to that of DB construction,
# Avoid double count of frequency with this option being false
incre_update = false

# Algorithm to decide frequency threshold
# [median, ] is available
threshold_mode = median

# Threshold value
# If mode is median, threshold means ratio of description words to all words
threshold = 0.6


[log_template_shiso]

# Threshold for SeqRatio in Search Phase
ltgen_threshold = 0.9

# Max child size of 1 node of tree in Search Phase
ltgen_max_child = 4

# Size of Ngram in Adjustment Phase
# If not ignoring splitter symbols, recommended to set more than 5
ltgroup_ngram_length = 3

# Lookup threshold for Ngram in Adjustment Phase
ltgroup_th_lookup = 0.3

# Threshold for edit distance in Adjustment Phase
ltgroup_th_distance = 0.85

# Keep found ngram database on memory
ltgroup_mem_ngram = true


[log_template_crf]

train_filename = crf_train
feature_template =
model_filename = crf_model
middle_label = re



[dag]
# IDs for grouping criterion of log messages to define DAG node
# [ltgid, ltid] is available
event_gid = ltgid

# Target term of log messages to construct DAG
# Divided into unit terms by "default_term" before processing
# If empty, set whole days including all log data in DB
# For example:
# whole_term = 2112-09-01 00:00:00, 2112-10-01 00:00:00
whole_term = 

# Target areas of DAG construction
# If "all" given, use whole hosts as 1 area named "all"
# If "each" given, define each host as 1 area named with hostname
# For example:
# area = core, area1, area2, area3
area = all

# Length of unit terms to construct DAG
unit_term = 30h

# Length of time difference of unit terms
unit_diff = 24h

# Bin size of discrete data for G square test
stat_bin = 10s

# Overlapping bin size (addtional to dag.stat_bin)
stat_bin_overlap = 0s

# Method to estimate conditional independency
# [fisherz, fisherz_bin, gsq, gsq_rlib] is available
ci_func = gsq

# for debugging
skeleton_verbose = false

# Method to estimate skeleton in PC algorithm
# default : original-PC algorithm, fast but not accurate in sparse data
# stable : stable-PC algorithm, result is order-independent of input data
skeleton_method = default

# Maximum depth of conditional independence
# if -1, no limit is set
skeleton_depth = -1

# Threshold of p-value for conditional independence test
threshold = 0.01
threshold_corr = 0.5

# Event definition data
event_dir = ev_output

# Found DAG object data
output_dir = pc_output

# Filtering redundant events with periodicity test before generating causality graph
usefilter = false

## use_filter : outdated


[changepoint]

temp_cp_data = cp_temp

# 1 bin must be able to divide 1 day
#cf_bin = 10s
cf_bin = 1m

#cf_r = 0.0001
cf_r = 0.001

cf_smooth = 5



[filter]
# Filters for log events
# [file, periodic, self-corr, periodic-whole] is available
# periodic : Filtering events that appear with stable interval
# self-corr : Filtering events that have high self-correlation coefficient
# periodic-whole : 'periodic' with the interval calculated with 'self-corr'

# Action to apply for periodic events
# [remove, replace, remove-corr, (to be added)]
action = remove

# Sampling term to calculate interval candidate in 'periodic-whole'
# If empty, use whole log data in DB
#sampling_term = 1w, 1d
dt_cond = 1d_1s, 7d_1m

# Required times of periodic event appearance to repeat
# Used for 'periodic' and 'self-corr'
# set more than 3
periodic_count = 5

# Required term length of periodic event to repeat
# Used for 'periodic' and 'self-corr'
periodic_term = 6h

# threshold for fourier analysis
threshold_spec = 0.4
threshold_eval = 0.1
threshold_restore = 0.5

# threshold for continuous distribution dt filter
count_linear = 10
binsize_linear = 10s
threshold_linear = 0.5


# for remove-corr (outdated)
#periodic_th = 0.5
self_corr_th = 0.5
self_corr_diff = 1h, 1d
#search_interval = true
#self_corr_bin = 10s
#seq_error = 0.01
#seq_duplication = false
#replace_top = true
#replace_end = true
#sampling_term = 1w, 1d


[visual]

# Log template label definition
# If empty, use default configuration (lt_label.conf.sample)
ltlabel = 

ltlabel_default_label = None
ltlabel_default_group = None


# If true, remove frequently appearing edges from the result
filter_edges = false
edge_filter_method = count_ighost
edge_filter_th = 0.1
edge_filter_file = edge.temp


[search]

temp_fn = .cg_temp

# [log, dag_ed, dag_mcs]
method = log

dag_ig_direction = true

dag_weight = true

